"""
Audio Manager
Low-latency audio playback layer with continuous streaming.

We decode the game's vocal clips with SoundFile and feed them through a
sounddevice OutputStream so that playback starts instantly and restarts without
hiccups whenever the animation loops.
"""

from __future__ import annotations

import os
import threading
from typing import Optional, Tuple

import numpy as np
import sounddevice as sd
import soundfile as sf
from PyQt6.QtCore import QObject, QCoreApplication, QThread, pyqtSignal


class _TimeStretchWorker(QObject):
    finished = pyqtSignal(int, object, float, bool)
    failed = pyqtSignal(int, str)

    def __init__(self, request_id: int, audio: np.ndarray, speed: float,
                 ratio: float, resume: bool):
        super().__init__()
        self.request_id = request_id
        self.audio = audio
        self.speed = speed
        self.ratio = ratio
        self.resume = resume

    def run(self):
        result = AudioManager._time_stretch_audio_static(self.audio, self.speed)
        self.finished.emit(self.request_id, result, self.ratio, self.resume)


class AudioManager(QObject):
    """Manages monster audio playback with manual loop control."""

    def __init__(self, parent: Optional[QObject] = None):
        super().__init__(parent)
        self._enabled: bool = True
        self._current_file: Optional[str] = None

        self._audio_data: Optional[np.ndarray] = None  # Processed audio buffer
        self._source_audio_data: Optional[np.ndarray] = None  # Original audio buffer
        self._sample_rate: int = 44100
        self._channels: int = 1
        self._duration: float = 0.0

        self._current_position: float = 0.0  # seconds
        self._playback_cursor: int = 0       # frame index
        self._play_active: bool = False

        self._volume: float = 0.8
        self._playback_speed: float = 1.0
        self._pitch_mode: str = "time_stretch"  # time_stretch, pitch_shift, chipmunk
        self._pending_pitch_config: Optional[Tuple[float, str]] = None
        self._stretch_thread: Optional[QThread] = None
        self._stretch_worker: Optional[_TimeStretchWorker] = None
        self._stretch_request_id: int = 0

        self._stream: Optional[sd.OutputStream] = None
        self._stream_lock = threading.RLock()
        self._time_stretch_ratio: float = 1.0

    # ------------------------------------------------------------------ #
    # Properties                                                         #
    # ------------------------------------------------------------------ #
    @property
    def current_file(self) -> Optional[str]:
        return self._current_file

    @property
    def is_ready(self) -> bool:
        return self._audio_data is not None

    # ------------------------------------------------------------------ #
    # Public controls                                                    #
    # ------------------------------------------------------------------ #
    def set_enabled(self, enabled: bool):
        if self._enabled == enabled:
            return
        self._enabled = enabled
        if not enabled:
            self._stop_playback()
            self._close_stream()

    def load_file(self, file_path: str) -> bool:
        """
        Load an audio clip and prepare it for playback.
        """
        if not file_path or not os.path.exists(file_path):
            self._reset_state()
            return False

        data, sample_rate = sf.read(file_path, always_2d=True, dtype="float32")

        self._stop_playback()
        self._close_stream()

        self._audio_data = np.ascontiguousarray(data, dtype=np.float32)
        self._source_audio_data = self._audio_data.copy()
        self._sample_rate = sample_rate
        self._channels = data.shape[1]
        self._duration = len(data) / sample_rate if sample_rate > 0 else 0.0
        self._current_file = file_path
        self._time_stretch_ratio = 1.0
        self._pitch_mode = "time_stretch"
        self._playback_speed = 1.0
        self._set_position(0.0)
        return True

    def play(self, start_time: Optional[float] = None):
        """Begin playback, optionally seeking first."""
        if not self._enabled or not self.is_ready:
            return

        if start_time is not None:
            self._set_position(start_time)

        if self._current_position >= self._duration:
            self._set_position(0.0)

        self._start_playback()

    def pause(self):
        """Pause playback but retain current position."""
        if not self.is_ready:
            return
        self._stop_playback()

    def stop(self):
        """Stop playback and rewind to start."""
        if not self.is_ready:
            return
        self._stop_playback()
        self._set_position(0.0)

    def restart(self):
        """Restart playback from the beginning."""
        if not self.is_ready:
            return
        self._stop_playback()
        self._set_position(0.0)
        if self._enabled:
            self._start_playback()

    def seek(self, seconds: float):
        """Seek to a new timestamp."""
        if not self.is_ready:
            return
        was_playing = self.is_playing()
        self._stop_playback()
        self._set_position(seconds)
        if was_playing and self._enabled:
            self._start_playback()

    def set_volume(self, volume_percent: int):
        """Adjust playback volume (0-100)."""
        clamped = max(0, min(100, volume_percent))
        with self._stream_lock:
            self._volume = clamped / 100.0
            stream = self._stream
        if stream:
            try:
                stream.write_available  # noop, ensures stream exists
            except Exception:
                pass

    def is_playing(self) -> bool:
        return self._play_active

    def configure_playback(self, speed: float, pitch_mode: str):
        """
        Apply tempo and pitch settings.

        Args:
            speed: Tempo multiplier relative to original audio.
            pitch_mode: 'time_stretch', 'pitch_shift', or 'chipmunk'.
        """
        clamped_speed = max(0.01, float(speed))
        if pitch_mode not in ("time_stretch", "pitch_shift", "chipmunk"):
            pitch_mode = "time_stretch"

        self._pending_pitch_config = (clamped_speed, pitch_mode)
        with self._stream_lock:
            source = self._source_audio_data
            current_time = self._current_position
            was_playing = self._play_active
            current_duration = self._duration
            if pitch_mode == "time_stretch" and self._pitch_mode == "time_stretch":
                effective_speed = self._time_stretch_ratio
            elif pitch_mode == self._pitch_mode:
                effective_speed = self._playback_speed
            else:
                effective_speed = None
            no_change = (
                source is not None
                and effective_speed is not None
                and abs(clamped_speed - effective_speed) < 1e-6
            )
        if source is None or no_change:
            self._pending_pitch_config = None
            return

        if pitch_mode == "time_stretch":
            self._start_time_stretch(source, clamped_speed, current_time, current_duration, was_playing)
        else:
            self._apply_pitch_mode(source, clamped_speed, pitch_mode, current_time, was_playing)

    # ------------------------------------------------------------------ #
    # Internal helpers                                                   #
    # ------------------------------------------------------------------ #
    def _reset_state(self):
        self._stop_playback()
        self._close_stream()
        self._cancel_stretch_worker()
        self._audio_data = None
        self._source_audio_data = None
        self._current_file = None
        self._duration = 0.0
        self._current_position = 0.0
        self._playback_cursor = 0
        self._time_stretch_ratio = 1.0
        self._playback_speed = 1.0
        self._pitch_mode = "time_stretch"

    def _set_position(self, seconds: float):
        target = self._clamp_time(seconds)
        with self._stream_lock:
            self._current_position = target
            if self._audio_data is not None and self._sample_rate > 0:
                frame = min(int(target * self._sample_rate), len(self._audio_data))
            else:
                frame = 0
            self._playback_cursor = frame

    def _start_playback(self):
        with self._stream_lock:
            if not self._audio_data is None:
                self._playback_cursor = min(self._playback_cursor, len(self._audio_data))
                self._play_active = True
        self._ensure_stream()

    def _stop_playback(self):
        with self._stream_lock:
            self._play_active = False

    def _ensure_stream(self):
        if self._stream is None and self.is_ready:
            self._stream = sd.OutputStream(
                samplerate=self._sample_rate,
                channels=self._channels,
                dtype="float32",
                callback=self._stream_callback,
            )
            self._stream.start()
        elif self._stream and not self._stream.active:
            self._stream.start()

    def _close_stream(self):
        if self._stream:
            try:
                self._stream.stop()
            except Exception:
                pass
            try:
                self._stream.close()
            except Exception:
                pass
            self._stream = None

    def _stream_callback(self, outdata, frames, time_info, status):
        with self._stream_lock:
            audio = self._audio_data
            if (
                not self._enabled
                or not self._play_active
                or audio is None
                or self._playback_cursor >= len(audio)
            ):
                outdata.fill(0.0)
                if (
                    self._play_active
                    and audio is not None
                    and self._playback_cursor >= len(audio)
                ):
                    self._play_active = False
                    self._current_position = self._duration
                return

            start = self._playback_cursor
            mode = self._pitch_mode

            if mode == "time_stretch":
                end = min(start + frames, len(audio))
                chunk = audio[start:end]
                produced = chunk.shape[0]
                if produced > 0:
                    if self._volume != 1.0:
                        np.multiply(chunk, self._volume, out=outdata[:produced])
                    else:
                        outdata[:produced] = chunk
                    if produced < frames:
                        outdata[produced:frames] = 0.0
                else:
                    outdata.fill(0.0)
                self._playback_cursor = end
                if self._sample_rate > 0:
                    self._current_position = min(
                        self._playback_cursor / self._sample_rate,
                        self._duration,
                    )
                if self._playback_cursor >= len(audio):
                    self._play_active = False
                    self._current_position = self._duration
                return

            speed = max(0.01, self._playback_speed)
            source = self._source_audio_data if self._source_audio_data is not None else audio
            if mode == "pitch_shift":
                processed, consumed = self._render_pitch_shift_chunk(source, start, frames, speed)
            else:
                processed, consumed = self._render_chipmunk_chunk(source, start, frames, speed)

            produced = processed.shape[0]
            if produced < frames:
                outdata.fill(0.0)
            if produced > 0:
                length = min(frames, produced)
                if self._volume != 1.0:
                    np.multiply(processed[:length], self._volume, out=outdata[:length])
                else:
                    outdata[:length] = processed[:length]
                np.clip(outdata[:length], -1.0, 1.0, out=outdata[:length])
                if length < frames:
                    outdata[length:] = 0.0
            else:
                outdata.fill(0.0)

            if consumed <= 0 or start >= len(audio):
                self._play_active = False
                self._playback_cursor = len(audio)
                self._current_position = self._duration
                return

            self._playback_cursor = min(start + consumed, len(audio))
            if self._playback_cursor >= len(audio):
                self._play_active = False
                self._current_position = self._duration
            elif self._sample_rate > 0:
                self._current_position = min(
                    self._playback_cursor / self._sample_rate,
                    self._duration,
                )

    def _clamp_time(self, value: float) -> float:
        if self._duration <= 0.0:
            return 0.0
        return max(0.0, min(self._duration, value))

    def export_audio_segment(
        self,
        duration: float,
        *,
        speed: Optional[float] = None,
        pitch_mode: Optional[str] = None,
    ) -> Optional[Tuple[np.ndarray, int]]:
        """
        Return audio rendered for the requested duration using the current tempo/pitch settings.

        Args:
            duration: Length in seconds to render.
            speed: Optional override for playback speed multiplier.
            pitch_mode: Optional override for pitch handling ('time_stretch', 'pitch_shift', 'chipmunk').
        """
        if not self.is_ready or duration <= 0.0:
            return None

        frames_needed = int(duration * self._sample_rate)
        if frames_needed <= 0:
            return None

        mode = pitch_mode or self._pitch_mode or "time_stretch"
        if mode not in ("time_stretch", "pitch_shift", "chipmunk"):
            mode = "time_stretch"

        if mode == "time_stretch":
            audio = self._audio_data
            if audio is None:
                return None
            if frames_needed <= len(audio):
                segment = audio[:frames_needed]
            else:
                pad_length = frames_needed - len(audio)
                pad = np.zeros((pad_length, self._channels), dtype=audio.dtype)
                segment = np.concatenate((audio, pad), axis=0)
            return segment.copy(), self._sample_rate

        source = self._source_audio_data if self._source_audio_data is not None else self._audio_data
        if source is None:
            return None
        effective_speed = max(0.01, float(speed if speed is not None else self._playback_speed))
        if mode == "pitch_shift":
            samples, _ = self._render_pitch_shift_chunk(source, 0, frames_needed, effective_speed)
        else:
            samples, _ = self._render_chipmunk_chunk(source, 0, frames_needed, effective_speed)
        return samples.copy(), self._sample_rate

    def _apply_pitch_mode(self, source: np.ndarray, speed: float, mode: str, seek_time: float, was_playing: bool):
        """Apply realtime resampling pitch modes."""
        if was_playing:
            self._stop_playback()
        with self._stream_lock:
            self._audio_data = source
            self._duration = len(source) / self._sample_rate if self._sample_rate > 0 else 0.0
            self._pitch_mode = mode
            self._playback_speed = speed
            if mode == "time_stretch":
                self._time_stretch_ratio = speed
            else:
                self._time_stretch_ratio = 1.0
            self._set_position(seek_time)
        if self._pending_pitch_config and self._pending_pitch_config[1] == mode:
            self._pending_pitch_config = None
        if was_playing and self._enabled:
            self._start_playback()

    def _start_time_stretch(self, source: np.ndarray, speed: float, current_time: float,
                            current_duration: float, was_playing: bool):
        """Start asynchronous time-stretch processing."""
        if abs(speed - 1.0) < 1e-6:
            self._apply_pitch_mode(source, 1.0, "time_stretch", current_time, was_playing)
            return

        self._cancel_stretch_worker()
        ratio = current_time / current_duration if current_duration > 1e-6 else 0.0
        new_request_id = self._stretch_request_id + 1
        self._stretch_request_id = new_request_id
        self._time_stretch_ratio = speed
        self._playback_speed = speed

        worker = _TimeStretchWorker(new_request_id, source.copy(), speed, ratio, was_playing)
        thread = QThread(self)
        worker.moveToThread(thread)
        thread.started.connect(worker.run)
        worker.finished.connect(self._on_stretch_finished)
        worker.finished.connect(worker.deleteLater)
        worker.failed.connect(self._on_stretch_failed)
        worker.failed.connect(worker.deleteLater)
        thread.finished.connect(thread.deleteLater)
        self._stretch_worker = worker
        self._stretch_thread = thread
        thread.start()

    def _cancel_stretch_worker(self):
        thread = self._stretch_thread
        worker = self._stretch_worker
        self._stretch_thread = None
        self._stretch_worker = None
        if thread:
            thread.quit()
            thread.wait()
        if worker:
            try:
                worker.deleteLater()
            except RuntimeError:
                pass

    def _on_stretch_finished(self, request_id: int, data_obj, ratio: float, resume: bool):
        if request_id != self._stretch_request_id:
            return
        stretched = np.ascontiguousarray(data_obj, dtype=np.float32)
        if resume:
            self._stop_playback()
        with self._stream_lock:
            self._audio_data = stretched
            self._duration = len(stretched) / self._sample_rate if self._sample_rate > 0 else 0.0
            self._pitch_mode = "time_stretch"
            self._playback_speed = self._time_stretch_ratio
            new_time = ratio * self._duration
            self._set_position(new_time)
        self._pending_pitch_config = None
        if resume and self._enabled:
            self._start_playback()
        self._cancel_stretch_worker()

    def _on_stretch_failed(self, request_id: int, message: str):
        if request_id != self._stretch_request_id:
            return
        print(f"Time-stretch failed: {message}")
        self._pending_pitch_config = None
        self._time_stretch_ratio = 1.0
        self._cancel_stretch_worker()

    def _render_pitch_shift_chunk(
        self,
        audio: np.ndarray,
        start: int,
        frames: int,
        speed: float
    ) -> Tuple[np.ndarray, int]:
        """Produce audio samples when pitch should follow speed changes."""
        if frames <= 0 or start >= len(audio):
            return np.zeros((0, self._channels), dtype=np.float32), 0

        positions = start + np.arange(frames, dtype=np.float32) * speed
        lower = np.floor(positions).astype(int)
        upper = lower + 1
        valid_mask = lower < len(audio)

        lower = np.clip(lower, 0, len(audio) - 1)
        upper = np.clip(upper, 0, len(audio) - 1)
        fractions = (positions - lower).astype(np.float32)

        samples = (audio[lower] * (1.0 - fractions)[:, None]) + (audio[upper] * fractions[:, None])
        if not np.all(valid_mask):
            samples[~valid_mask] = 0.0

        consumed = int(np.ceil(frames * speed))
        consumed = max(1, min(consumed, len(audio) - start))
        return samples.astype(np.float32, copy=False), consumed

    def _render_chipmunk_chunk(
        self,
        audio: np.ndarray,
        start: int,
        frames: int,
        speed: float
    ) -> Tuple[np.ndarray, int]:
        """
        Generate an exaggerated pitch-shifted sound (helium/chipmunk effect) while
        keeping tempo synced to BPM. We apply a pitch shift and then re-map the
        playback cursor to the normal tempo progression.
        """
        if frames <= 0 or start >= len(audio):
            return np.zeros((0, self._channels), dtype=np.float32), 0

        extra = 1.35
        effective_speed = speed * extra if speed >= 1.0 else speed / extra
        samples, consumed_eff = self._render_pitch_shift_chunk(audio, start, frames, effective_speed)

        consumed = int(np.ceil(frames * speed))
        consumed = max(1, min(consumed, len(audio) - start))
        return samples, consumed

    @staticmethod
    def _time_stretch_audio_static(audio: np.ndarray, speed: float) -> np.ndarray:
        """Resize audio length using linear interpolation to preserve pitch."""
        if speed <= 0:
            speed = 0.01
        if abs(speed - 1.0) < 1e-6:
            return audio.copy()

        frame = 2048
        hop_a = frame // 2
        hop_s = max(1, int(round(hop_a / speed)))
        window = np.hanning(frame).astype(np.float32)
        channels = audio.shape[1]

        estimated_len = int(len(audio) / speed) + frame * 2
        output = np.zeros((estimated_len, channels), dtype=np.float32)
        window_norm = np.zeros(estimated_len, dtype=np.float32)

        pos = 0
        write = 0
        while pos < len(audio) and write < estimated_len - frame:
            chunk = audio[pos:pos + frame]
            if chunk.shape[0] < frame:
                pad = np.zeros((frame, channels), dtype=np.float32)
                pad[:chunk.shape[0]] = chunk
                chunk = pad
            windowed = chunk * window[:, None]
            output[write:write + frame] += windowed
            window_norm[write:write + frame] += window
            pos += hop_a
            write += hop_s

        nonzero = window_norm > 1e-3
        output[nonzero] /= window_norm[nonzero][:, None]
        max_index = np.where(window_norm > 0)[0]
        if max_index.size > 0:
            last_index = min(len(output), max_index[-1] + frame)
            output = output[:last_index]
        return output

    @staticmethod
    def _resample_chunk(chunk: np.ndarray, new_length: int) -> np.ndarray:
        """Simple linear resampler to preserve pitch."""
        if new_length <= 0:
            return np.zeros((0, chunk.shape[1]), dtype=np.float32)
        if len(chunk) == 0:
            return np.zeros((new_length, chunk.shape[1]), dtype=np.float32)
        if len(chunk) == 1:
            return np.repeat(chunk, new_length, axis=0)
        positions = np.linspace(0, len(chunk) - 1, new_length)
        idx_lower = np.floor(positions).astype(int)
        idx_upper = np.clip(idx_lower + 1, 0, len(chunk) - 1)
        frac = (positions - idx_lower)[:, None]
        resampled = chunk[idx_lower] * (1.0 - frac) + chunk[idx_upper] * frac
        return resampled.astype(np.float32, copy=False)

    def clear(self):
        """Fully reset and release any loaded audio."""
        self._reset_state()
